{"cells":[{"cell_type":"markdown","metadata":{"id":"wl4W3uFk9pmd"},"source":["**The Idea is to generate poem text using shakespear data**"]},{"cell_type":"markdown","metadata":{"id":"CLQ8zC3Q9396"},"source":["# Import libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4363,"status":"ok","timestamp":1687586152559,"user":{"displayName":"sudheer vanapalli","userId":"06811914796989901949"},"user_tz":-330},"id":"XMEWODra-F5O","outputId":"7cab12c2-f3e9-40b6-b431-6dec917f4dfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.12.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1687586297116,"user":{"displayName":"sudheer vanapalli","userId":"06811914796989901949"},"user_tz":-330},"id":"dWKl4VgI-pAe"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"owjQS11F-LrB"},"source":["# Get shakespear data"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1687586458993,"user":{"displayName":"sudheer vanapalli","userId":"06811914796989901949"},"user_tz":-330},"id":"FlqBdLc--XTQ","outputId":"6d6f72ad-779e-4613-ede2-0a01fd55790e"},"outputs":[{"name":"stdout","output_type":"stream","text":["First Citizen:\n","Before we proceed any further, hear\n"]}],"source":["shakespeare_url = 'https://homl.info/shakespeare'\n","shakespeare_filepath = tf.keras.utils.get_file('shakespeare.txt', shakespeare_url)\n","\n","with open(shakespeare_filepath) as f:\n","  shakespeare_text = f.read()\n","\n","print(shakespeare_text[:50])"]},{"cell_type":"markdown","metadata":{"id":"zenB9E-_-Zq2"},"source":["#  Build model"]},{"cell_type":"markdown","metadata":{"id":"HRLFCKXj-aIm"},"source":["## Text Vectorization"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1722,"status":"ok","timestamp":1687587054893,"user":{"displayName":"sudheer vanapalli","userId":"06811914796989901949"},"user_tz":-330},"id":"6pVs1fuM-aWh","outputId":"6201d731-6404-437a-ebd1-2b625c50c292"},"outputs":[{"data":{"text/plain":["\u003ctf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12])\u003e"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n","\n","text_vec_layer.adapt([shakespeare_text])\n","\n","encoded = text_vec_layer([shakespeare_text])[0]\n","\n","encoded"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1687587067423,"user":{"displayName":"sudheer vanapalli","userId":"06811914796989901949"},"user_tz":-330},"id":"Tf8iLabyBp5j"},"outputs":[],"source":["encoded -=2 # To remove padded tokens 0,1"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1687587137359,"user":{"displayName":"sudheer vanapalli","userId":"06811914796989901949"},"user_tz":-330},"id":"y2Tm9RxPBeTq","outputId":"01b12963-d022-4a6b-f5fd-1e47f1a6774b"},"outputs":[{"data":{"text/plain":["39"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["dataset_size = len(encoded)\n","\n","n_tokens = text_vec_layer.vocabulary_size() - 2\n","\n","n_tokens"]},{"cell_type":"markdown","metadata":{"id":"lyzSowgD-aup"},"source":["## Make windows of text, shuffle and batch them"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":413,"status":"ok","timestamp":1687588144131,"user":{"displayName":"sudheer vanapalli","userId":"06811914796989901949"},"user_tz":-330},"id":"Zy5nXzpw-a5s"},"outputs":[],"source":["def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n","\n","  ds = tf.data.Dataset.from_tensor_slices(sequence) # get the sequence\n","\n","  ds = ds.window(length + 1, shift=1, drop_remainder=True) # partition them into windows, we add extra 1 because we need to predict a character target\n","\n","  ds = ds.flat_map(lambda window_ds: window_ds.batch(length+1)) # make a flatmap batch with window size\n","\n","  if shuffle:\n","    ds = ds.shuffle(buffer_size=100_000, seed=seed)\n","\n","  ds = ds.batch(batch_size) # batch them\n","\n","  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1) # split into input, output pairs and activate prefetch\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BkpDNNRrF34i"},"source":["## Generate train, vadlidation, text data"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1687589629169,"user":{"displayName":"sudheer vanapalli","userId":"06811914796989901949"},"user_tz":-330},"id":"8CTkdKwdF9pN"},"outputs":[],"source":["length = 100 # length of each window\n","\n","tf.random.set_seed(42)\n","\n","train_set = to_dataset(encoded[:1000000], length=length, shuffle=True, seed=42)\n","\n","validation_set = to_dataset(encoded[1000000: 1060000], length=length)\n","\n","test_set = to_dataset(encoded[1060000:], length=length)"]},{"cell_type":"markdown","metadata":{"id":"0CmXy8_4Lic9"},"source":["## Build core model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"FxjcPjLWLtqX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","     66/Unknown - 32s 124ms/step - loss: 3.2223 - accuracy: 0.1453"]}],"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(input_dim= n_tokens, output_dim= 16),\n","    tf.keras.layers.GRU(128, return_sequences=True),\n","    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n","  ], name=\"shakespeare_poem\")\n","\n","model.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=\"nadam\",\n","    metrics=[\"accuracy\"]\n","    )\n","\n","model_chkpt = tf.keras.callbacks.ModelCheckpoint(\n","    \"shakespeare_poem\",\n","    monitor=\"val_accuracy\",\n","    save_best_only=True\n","    )\n","\n","history = model.fit(\n","    train_set,\n","    validation_data=validation_set,\n","    epochs=10,\n","    callbacks=[model_chkpt]\n","    )\n","\n","# Final model with text preprocessing\n","\n","shakespeare_model = tf.keras.Sequential([\n","    text_vec_layer,\n","    tf.keras.layers.Lambda(lambda x: x -2),\n","    model\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRX2qJV1PZYQ"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPjLlz6n7v7hkkmvdfUeKEQ","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}